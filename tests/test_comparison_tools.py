#!/usr/bin/env python3
"""Test all comparison tools comprehensively."""

from mcp.config import MCPConfig
from mcp.server import SimulationMCPServer


def print_section(title):
    print(f"\n{'='*70}")
    print(f"  {title}")
    print('='*70)


def test_comparison_tools():
    """Test all 4 comparison tools."""
    
    # Initialize server
    print_section("SERVER INITIALIZATION")
    config = MCPConfig.from_db_path("/workspace/simulation.db")
    server = SimulationMCPServer(config)
    print(f"âœ“ Server initialized with {len(server.list_tools())} tools")
    
    # Get simulation IDs for testing
    list_sims = server.get_tool("list_simulations")
    sims_result = list_sims(limit=10)
    sim_ids = [s['simulation_id'] for s in sims_result['data']['simulations']]
    
    print(f"âœ“ Found {len(sim_ids)} simulations for comparison")
    
    # Test 1: Compare Simulations
    print_section("1. COMPARE SIMULATIONS")
    
    if len(sim_ids) >= 2:
        compare_sims = server.get_tool("compare_simulations")
        result = compare_sims(simulation_ids=sim_ids[:2])
        
        print(f"âœ“ Success: {result['success']}")
        print(f"âœ“ Execution time: {result['metadata']['execution_time_ms']:.2f}ms")
        
        if result['success']:
            print(f"\nComparing {result['data']['simulation_count']} simulations")
            print(f"Metrics compared: {', '.join(result['data']['metrics_compared'])}")
            
            # Show simulation results
            for sim_id, metrics in list(result['data']['simulations'].items())[:2]:
                print(f"\n{sim_id}:")
                for metric, stats in list(metrics.items())[:3]:
                    if stats:
                        print(f"  {metric}:")
                        print(f"    Mean: {stats['mean']:.2f}")
                        print(f"    Range: {stats['min']:.2f} - {stats['max']:.2f}")
            
            # Show rankings
            if 'rankings' in result['data']:
                print(f"\nRankings by total_agents:")
                for entry in result['data']['rankings'].get('total_agents', [])[:3]:
                    print(f"  Rank {entry['rank']}: {entry['simulation_id']} = {entry['value']}")
    else:
        print("âš  Need at least 2 simulations for comparison")
        # Create a single-sim comparison for testing
        compare_sims = server.get_tool("compare_simulations")
        # This should fail validation
        try:
            result = compare_sims(simulation_ids=[sim_ids[0]])
            print(f"âœ— Should have failed validation")
        except Exception as e:
            print(f"âœ“ Properly validates minimum 2 simulations required")
    
    # Test 2: Compare Parameters
    print_section("2. COMPARE PARAMETERS")
    compare_params = server.get_tool("compare_parameters")
    
    # Test with a common parameter
    result = compare_params(
        parameter_name="simulation_steps",
        outcome_metric="total_agents",
        limit=10
    )
    
    print(f"âœ“ Success: {result['success']}")
    if 'execution_time_ms' in result['metadata']:
        print(f"âœ“ Execution time: {result['metadata']['execution_time_ms']:.2f}ms")
    else:
        print(f"âœ“ Execution time: N/A (error case)")
    
    if result['success']:
        if 'groups' in result['data']:
            print(f"\nParameter: {result['data']['parameter']}")
            print(f"Outcome metric: {result['data']['outcome_metric']}")
            print(f"Groups found: {result['data']['groups_count']}")
            print(f"Total simulations: {result['data']['total_simulations_analyzed']}")
            
            # Show groups
            for param_value, group_data in list(result['data']['groups'].items())[:3]:
                print(f"\n  Parameter value: {param_value}")
                print(f"    Simulations: {group_data['simulation_count']}")
                print(f"    Group mean: {group_data['group_mean']:.2f}")
                if group_data.get('best_simulation'):
                    best = group_data['best_simulation']
                    print(f"    Best: {best['simulation_id']} (mean={best['mean']:.2f})")
        else:
            print(f"  {result['data'].get('message', 'No groups')}")
    
    # Test 3: Rank Configurations
    print_section("3. RANK CONFIGURATIONS")
    rank_configs = server.get_tool("rank_configurations")
    
    result = rank_configs(
        metric_name="total_agents",
        aggregation="mean",
        limit=10
    )
    
    print(f"âœ“ Success: {result['success']}")
    if 'execution_time_ms' in result['metadata']:
        print(f"âœ“ Execution time: {result['metadata']['execution_time_ms']:.2f}ms")
    
    if result['success']:
        print(f"\nRanking by: {result['data']['metric']} ({result['data']['aggregation']})")
        print(f"Total ranked: {result['data']['total_ranked']}")
        
        if result['data']['rankings']:
            print(f"\nTop 5 configurations:")
            for entry in result['data']['rankings'][:5]:
                print(f"  #{entry['rank']}: {entry['simulation_id']}")
                print(f"    Score: {entry['score']}")
                print(f"    Steps analyzed: {entry['steps_analyzed']}")
                # Show sample parameters
                params = entry['parameters']
                sample_params = {k: v for k, v in list(params.items())[:3]}
                print(f"    Parameters (sample): {sample_params}")
            
            # Show statistics
            if 'statistics' in result['data']:
                stats = result['data']['statistics']
                print(f"\nStatistics:")
                print(f"  Mean score: {stats['mean_score']:.2f}")
                print(f"  Std dev: {stats['std_score']:.2f}")
                print(f"  Range: {stats['score_range']['min']:.2f} - {stats['score_range']['max']:.2f}")
    
    # Test 4: Compare Generations
    print_section("4. COMPARE GENERATIONS")
    compare_gens = server.get_tool("compare_generations")
    
    result = compare_gens(simulation_id=sim_ids[0], max_generations=5)
    
    print(f"âœ“ Success: {result['success']}")
    print(f"âœ“ Execution time: {result['metadata']['execution_time_ms']:.2f}ms")
    
    if result['success']:
        print(f"\nTotal generations: {result['data']['total_generations']}")
        print(f"Analyzed: {result['data']['generations_analyzed']}")
        
        if 'summary' in result['data']:
            summary = result['data']['summary']
            print(f"Total agents: {summary['total_agents']}")
            print(f"Best survival generation: {summary['best_survival_generation']}")
        
        # Show generation details
        gens = result['data']['generations']
        print(f"\nGeneration Details:")
        for gen, stats in list(gens.items())[:5]:
            print(f"  Generation {gen}:")
            print(f"    Agents: {stats['total_agents']} (alive: {stats['alive']}, dead: {stats['dead']})")
            print(f"    Survival rate: {stats['survival_rate_percent']}%")
            if 'lifespan_stats' in stats:
                ls = stats['lifespan_stats']
                print(f"    Avg lifespan: {ls['mean']} (range: {ls['min']}-{ls['max']})")
    
    # Cache Statistics
    print_section("CACHE STATISTICS")
    stats = server.get_cache_stats()
    print(f"Cache size: {stats['size']}/{stats['max_size']}")
    print(f"Hits: {stats['hits']}, Misses: {stats['misses']}")
    if stats['hits'] + stats['misses'] > 0:
        print(f"Hit rate: {stats['hit_rate']:.1%}")
    
    print_section("âœ“ ALL COMPARISON TOOLS TESTED SUCCESSFULLY")
    print("Phase 4 comparison tools are fully functional:")
    print("  âœ“ compare_simulations - multi-simulation metric comparison")
    print("  âœ“ compare_parameters - parameter impact analysis")
    print("  âœ“ rank_configurations - performance ranking")
    print("  âœ“ compare_generations - evolutionary progression analysis")
    
    print(f"\nðŸŽ‰ Total tools now available: {len(server.list_tools())}")
    print("  - 4 Metadata tools")
    print("  - 6 Query tools")
    print("  - 7 Analysis tools")
    print("  - 4 Comparison tools")
    
    server.close()


if __name__ == "__main__":
    test_comparison_tools()